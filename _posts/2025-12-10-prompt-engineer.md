---
title: 'Prompt Engineering & LLM as a judge'
date: 2025-12-11
permalink: /posts/2025/12/llm-as-a-judge-test/
tags:
  - LLM
  - Prompt Engineering
  - LLM as a judge
header:
  teaser: prompt_engineer_rmse_comparison.png
---

关于提示词和LLM as a judge的一次测试和

2023年ChatGPT横空出世带火了Prompt Engineering，虽然大家已经意识到想要提升模型在特定任务上的表现上限，后训练必不可少。但即使是现在，Prompt Engineering因为其快速、低成本的特点，仍然是大家的go-to choice。如果prompt能解决，那不必上重武器了。

本人暑期实习的第一课就是写prompt，但虽然角色，context，分隔符，few-shot，chain-of-thought，格式限制这些技巧已经耳熟能详，但实际操作起来写出一个效果好的prompt还是挺难的。比如本次作业的leaderboard， rmse从0.83-1.4。

正好这学期EECS595的最后一次作业就是写prompt做llm as a judge，完成作业的过程也有不少收获。于是决定把报告记录在这。

# HW4 作文评分实验 - 最终报告

## 实验概述
本报告总结了使用多个语言模型和11种不同提示策略进行自动化作文评分实验的结果，旨在探索提示工程对作文评分性能的影响。

## 测试模型
- **HuggingFaceTB/SmolLM3-3B**：具有30亿参数的小型语言模型
- **Qwen/Qwen3-4B**：具有40亿参数的中型语言模型
- **ibm-granite/granite-4.0-micro**：针对评分任务优化的Granite模型

## 测试的提示策略
我们实现了11种多样化的提示策略，以系统性地探索不同的作文评分方法：

__Prompt 0 (Ultra-Minimalist)__: 极短，仅要求输出数字分数，用于评估最直接的评分能力。

__Prompt 1 (Short & Direct)__: 较短，直接请求分数，但包含作业和文章内容，比Prompt 0稍长。

__Prompt 2 (Standard Rubric)__: 中等长度，包含标准评分要素（相关性、语言、组织、支持），并要求JSON格式输出。

__Prompt 3 (Long Chain of Thought)__: 非常长且详细，引导模型进行多步骤、深度的分析过程，要求JSON输出，包含分析步骤和最终理由。

__Prompt 4 (Few-shot with Varied Examples)__: 较长，通过提供3个不同复杂度的评分示例来指导模型，要求JSON输出，包含分数和理由。

__Prompt 5 (Extremely Detailed Multi-criteria with Sub-scores)__: 非常长且最详细，要求模型对多个细化标准（内容、组织、语言、证据、批判性思维）分别评分，并给出加权总分和详细理由，要求JSON输出。

__Prompt 6 (Adversarial/Critique-focused)__: 中等长度，要求模型专注于识别文章的所有缺陷和错误，然后基于缺陷的严重性和数量进行评分，要求JSON输出，包含批评列表和影响总结。

__Prompt 7 (Audience/Purpose-focused)__: 中等长度，要求模型假想文章提交给学术期刊，并根据其是否符合学术要求来评分，要求JSON输出，包含适用性评估和建议。

__Prompt 8 (Contrastive/Comparative Scoring vs. Ideal)__: 中等长度，要求模型将文章与一个"理想"版本进行对比，识别优缺点，然后评分，要求JSON输出，包含对比笔记。

__Prompt 9 (Self-Correction/Reflection)__: 较长，要求模型先给出初步评分和理由，然后进行自我审视和修正，最终给出精炼后的评分和修正过程说明，要求JSON输出。

__Prompt 10 (Chinese Prompt)__: 中等长度的中文prompt，针对中文语境设计，要求JSON输出分数和理由，与英文prompt形成语言上的显著差异。

![123](/images/prompt_engineer_rmse_comparison.png)

| 模型 | 提示 | RMSE | MAE | 相关性 | 精确准确率 | 1分内准确率 |
|------|------|------|-----|--------|------------|-------------|
| SmolLM3-3B | 0 | 2.227 | 1.858 | 0.063 | 0.120 | 0.429 |
| SmolLM3-3B | 1 | 1.828 | 1.465 | 0.379 | 0.199 | 0.562 |
| SmolLM3-3B | 2 | 1.224 | 0.939 | 0.481 | 0.309 | 0.783 |
| SmolLM3-3B | 3 | 2.622 | 2.341 | NaN | 0.037 | 0.261 |
| SmolLM3-3B | 4 | 1.329 | 0.983 | 0.313 | 0.333 | 0.753 |
| SmolLM3-3B | 5 | 1.239 | 0.939 | 0.373 | 0.315 | 0.787 |
| SmolLM3-3B | 6 | 2.126 | 1.870 | 0.275 | 0.043 | 0.413 |
| SmolLM3-3B | 7 | 1.127 | 0.829 | 0.574 | 0.370 | 0.822 |
| SmolLM3-3B | 8 | 2.733 | 2.428 | 0.046 | 0.046 | 0.264 |
| SmolLM3-3B | 9 | 1.143 | 0.830 | 0.481 | 0.372 | 0.829 |
| SmolLM3-3B | 10 | 1.284 | 1.000 | 0.491 | 0.276 | 0.769 |
| Qwen3-4B | 0 | 2.622 | 2.340 | -0.065 | 0.039 | 0.261 |
| Qwen3-4B | 1 | 2.644 | 2.365 | -0.038 | 0.035 | 0.255 |
| Qwen3-4B | 2 | 2.441 | 2.144 | 0.039 | 0.059 | 0.324 |
| Qwen3-4B | 3 | 2.648 | 2.381 | -0.062 | 0.026 | 0.255 |
| Qwen3-4B | 4 | 2.085 | 1.720 | 0.080 | 0.166 | 0.446 |
| Qwen3-4B | 5 | 2.031 | 1.654 | 0.153 | 0.139 | 0.521 |
| Qwen3-4B | 6 | 2.413 | 2.137 | -0.036 | 0.036 | 0.326 |
| Qwen3-4B | 7 | 2.352 | 2.075 | -0.007 | 0.019 | 0.374 |
| Qwen3-4B | 8 | 2.741 | 2.467 | -0.013 | 0.031 | 0.229 |
| Qwen3-4B | 9 | 1.417 | 1.030 | 0.443 | 0.323 | 0.754 |
| Qwen3-4B | 10 | 2.483 | 2.000 | -0.202 | 0.125 | 0.458 |
| granite-4 | 0 | 2.627 | 2.345 | -0.053 | 0.037 | 0.262 |
| granite-4 | 1 | 1.254 | 0.964 | 0.416 | 0.310 | 0.756 |
| granite-4 | 2 | 1.144 | 0.868 | 0.489 | 0.332 | 0.820 |
| granite-4 | 3 | 2.623 | 2.342 | NaN | 0.037 | 0.261 |
| granite-4 | 4 | 1.396 | 1.110 | 0.545 | 0.241 | 0.711 |
| granite-4 | 5 | 1.192 | 0.894 | 0.491 | 0.341 | 0.791 |
| granite-4 | 6 | 1.818 | 1.463 | 0.234 | 0.203 | 0.538 |
| granite-4 | 7 | 1.045 | 0.765 | 0.483 | 0.386 | 0.862 |
| granite-4 | 8 | 1.086 | 0.813 | 0.465 | 0.358 | 0.841 |
| granite-4 | 9 | 1.046 | 0.769 | 0.467 | 0.379 | 0.867 |
| granite-4 | 10 | 1.325 | 1.058 | 0.483 | 0.253 | 0.726 |


## 最佳性能
**获胜者：granite-4.0-micro 使用提示7**
- RMSE：1.045
- 相关性：0.483
- 1分内准确率：0.862
- 精确准确率：0.386

**亚军：granite-4.0-micro 使用提示9**
- RMSE：1.046
- 相关性：0.467
- 1分内准确率：0.867
- 精确准确率：0.379

**季军：SmolLM3-3B 使用提示7**
- RMSE：1.127
- 相关性：0.574
- 1分内准确率：0.822
- 精确准确率：0.370

## 关键发现

### 模型性能分析
1. **Granite-4.0-micro 整体优势**：在所有33个实验组合中表现最稳定，获得最佳RMSE值（1.045）和最高1分内准确率（0.867）。该模型在大多数提示策略下都保持了正相关性，显示出强鲁棒性。

2. **SmolLM3-3B 高相关潜力**：小型模型展现出意外的高相关性潜力，特别是在提示7（受众/目的聚焦）上达到0.574的峰值相关性，表明小模型在特定提示策略下可能具有优势。

3. **Qwen3-4B 显著局限性**：表现出系统性问题，多数提示下产生负相关性，提示该模型可能不适合直接用于作文评分任务，需要进一步的任务特定微调。

### 提示工程洞察
1. **受众聚焦策略最有效**：提示7（学术期刊聚焦）在所有模型中都表现出色，granite-4.0-micro上达到0.862的1分内准确率。这表明明确指定评分上下文和受众能显著提升模型表现。

2. **自我修正机制提升精度**：提示9（自我修正/反思）在granite模型上实现第二佳性能，1分内准确率达0.867。模型先初步评分再反思修正的流程提升了评分精度。

3. **提示长度与性能非线性关系**：极简提示（提示0）和极长提示（提示3）都表现较差，而中等长度但针对性强的提示策略（如7、9）效果最佳。

4. **特定模型-提示组合效应**：最佳性能来自特定模型与提示策略的匹配，如SmolLM3-3B在提示7上相关性最高，而非granite模型。

5. **思维链提示的双刃剑效应**：提示3（思维链）在granite和SmolLM3上导致严重性能下降，但在Qwen3-4B上略有改善，表明该策略对模型架构敏感。

## Takeaway

这个作文评估实验虽小，但对我以后的工作有如下启发：

1. prompt写作：并不总是越长越好，反而中等的可以。中等的不行，更长的也没用。

2. 工作流：维护一个好的标注数据集（甚至数据版本）和一个好的eval pipeline可以避免盲目试。这大概是prompt engineer中engineer的部分。